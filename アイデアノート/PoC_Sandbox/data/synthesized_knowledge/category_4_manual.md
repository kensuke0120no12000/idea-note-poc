# 提案するカテゴリ名: **基盤サービス障害** 対応マニュアル\n# 基盤サービス障害 対応マニュアル

## 問題の概要

このカテゴリの障害は、システムの根幹を支える基盤サービス（データベース、ストレージ、メッセージングシステム、ネットワークなど）において発生する問題全般を指します。ログからは、主に以下の3つのパターンが確認できます。

1.  **データベース接続問題**: データベースへの接続タイムアウトが頻繁に発生し、API Gatewayや認証サービスなど、データベースに依存する上位サービスに影響を与えています。
2.  **ストレージ容量不足**: 特定のサーバーノード（`node-A`）でディスク容量が危機的に低い状態にあり、データベースサービスやAPI Gatewayの動作に支障をきたしています。
3.  **メッセージキュー処理エラー**: メッセージキューからのメッセージ処理が失敗しており、特に「`malformed JSON`」という形式不正が原因で、データベースサービスやAPI Gatewayに影響が出ています。

これらの問題は、単一のコンポーネントの障害に留まらず、システム全体の可用性、パフォーマンス、およびデータ整合性に深刻な影響を及ぼす可能性があります。

## 考えられる根本原因

提供されたログから推測される根本原因は以下の通りです。

1.  **データベース関連の根本原因**:
    *   **リソース枯渇**: データベースサーバーのCPU、メモリ、I/Oリソースが逼迫し、新しい接続の確立や既存接続の維持が困難になっている。
    *   **接続プールの枯渇**: アプリケーション側のデータベース接続プール設定が不適切、または急激なトラフィック増加により接続が枯渇している。
    *   **ネットワーク問題**: データベースサーバーとアプリケーションサーバー間のネットワーク遅延、パケットロス、または一時的なネットワーク障害。
    *   **データベースの過負荷**: スロークエリ、ロック競合、または大量のトランザクション処理により、データベース自体が応答不能になっている。
    *   **設定不備**: データベースのタイムアウト設定が短すぎる、または接続上限が低すぎる。

2.  **ストレージ関連の根本原因**:
    *   **ディスク容量の枯渇**: `node-A`におけるログファイル、一時ファイル、またはアプリケーションデータが肥大化し、ディスク容量が限界に達している。
    *   **I/O性能の低下**: ディスクI/Oがボトルネックとなり、データベースやアプリケーションのファイル操作が遅延している。
    *   **ファイルシステムの破損**: 稀にファイルシステム自体に問題が発生している可能性。

3.  **メッセージキュー関連の根本原因**:
    *   **データ形式の不整合**: メッセージを生成する側のアプリケーションが不正なJSON形式のメッセージをキューに送信している。
    *   **コンシューマの処理能力不足**: メッセージを消費する側のアプリケーションが、キューに溜まるメッセージ量に対して処理が追いついていない。
    *   **メッセージキューブローカーの問題**: メッセージキューブローカー自体のリソース枯渇、ネットワーク問題、または設定ミス。
    *   **コンシューマのバグ**: メッセージを消費するアプリケーションのJSONパースロジックにバグがある。

## ビジネスへの影響

これらの基盤サービス障害は、ビジネスに多岐にわたる深刻な影響を及ぼします。

*   **サービス停止/機能不全**: データベース接続不可やディスク容量不足は、ユーザー認証、データ保存、APIリクエスト処理など、サービスの主要機能を停止させ、最悪の場合、サービス全体のダウンタイムを引き起こします。
*   **ユーザー体験の著しい低下**: レスポンスタイムの増加、エラーページの頻繁な表示、操作の失敗などにより、ユーザーはサービスを快適に利用できなくなり、不満や離反につながります。
*   **データ損失/破損の可能性**: メッセージキュー処理の失敗が繰り返されると、重要なデータが適切に処理されず、データ損失やシステム間のデータ不整合が発生するリスクがあります。
*   **収益機会の損失**: サービスが利用できない、または機能が制限されることで、直接的な売上機会の損失が発生します。特にECサイトや決済システムでは、その影響は甚大です。
*   **ブランドイメージと信頼性の毀損**: 頻繁な障害は、企業の技術力や信頼性に対する顧客の認識を低下させ、長期的なブランド価値に悪影響を与えます。
*   **運用コストの増加**: 障害対応のための緊急リソース投入、原因究明のための時間と労力、そして恒久対策のための投資が必要となり、運用コストが増加します。

## 推奨される一次対応

問題発生時にエンジニアが最初に行うべき具体的な確認手順と応急処置を以下に示します。

1.  **アラートの確認とトリアージ**:
    *   発生しているアラートの詳細（サービス名、エラーメッセージ、エラーコード、発生頻度、影響度）を即座に確認し、緊急度と優先度を判断します。
    *   関連する監視ダッシュボード（Grafana, Datadogなど）で、CPU使用率、メモリ使用率、ディスクI/O、ネットワークトラフィック、データベース接続数、キューの滞留数などのメトリクスを異常値がないか確認します。

2.  **影響範囲の特定**:
    *   どのサービス（`api_gateway`, `auth_service`, `database`など）が影響を受けているか、またその依存関係を確認します。
    *   特定のサーバー（例: `node-A`）に問題が集中しているかを確認します。
    *   ユーザーからの問い合わせや、外部監視ツールからのアラートを確認し、ユーザー影響の有無と規模を把握します。

3.  **データベース接続タイムアウトの場合**:
    *   **データベースサーバーの健全性確認**:
        *   データベースサーバーにSSH接続し、`top`, `htop`, `iostat`, `vmstat`などでCPU、メモリ、ディスクI/Oの使用率を確認します。
        *   データベースのプロセスリスト（例: `SHOW PROCESSLIST` for MySQL, `pg_stat_activity` for PostgreSQL）を確認し、スロークエリやロック競合がないか確認します。
        *   データベースの接続数上限に達していないか確認します。
    *   **ネットワーク疎通確認**:
        *   アプリケーションサーバーからデータベースサーバーへの`ping`や`telnet`（データベースポート）でネットワーク疎通を確認します。
    *   **応急処置**:
        *   一時的にデータベースの接続プールサイズを増やす（可能であれば）。
        *   負荷の高いアプリケーションインスタンスを一時的にスケールアウトまたは再起動する（影響を考慮し慎重に）。
        *   データベースのリードレプリカがある場合、リードトラフィックをそちらに振り分ける。

4.  **ディスク容量不足の場合（`node-A`）**:
    *   **ディスク使用量の確認**:
        *   `node-A`にSSH接続し、`df -h`で各ファイルシステムの使用率を確認します。
        *   `du -sh /*`や`du -sh /var/log/*`などで、どのディレクトリが容量を消費しているか特定します。
    *   **応急処置**:
        *   不要なログファイル、古いバックアップファイル、一時ファイルなどを削除して、一時的にディスク容量を確保します。
        *   ログローテーション設定が適切か確認し、手動でローテーションを実行します。
        *   アプリケーションが大量のファイルを生成していないか確認し、一時的に出力を抑制します。

5.  **メッセージキュー処理エラーの場合**:
    *   **メッセージキューブローカーの健全性確認**:
        *   メッセージキューブローカー（Kafka, RabbitMQ, SQSなど）の管理コンソールや監視ツールで、ブローカーの稼働状況、キューの滞留数、エラーレートを確認します。
    *   **メッセージ内容の確認**:
        *   可能であれば、キューに滞留しているメッセージの一部を取得し、`malformed JSON`の原因となっている不正な形式を特定します。
    *   **コンシューマアプリケーションの確認**:
        *   メッセージを消費しているアプリケーションのログを確認し、JSONパースエラーの詳細やスタックトレースを確認します。
        *   コンシューマアプリケーションのリソース使用率（CPU, メモリ）を確認し、処理能力が不足していないか確認します。
    *   **応急処置**:
        *   不正なメッセージが原因で処理がブロックされている場合、そのメッセージをデッドレターキュー（DLQ）に移動させるか、スキップする。
        *   コンシューマアプリケーションのインスタンスを一時的にスケールアウトする。
        *   メッセージを生成する側のアプリケーションに問題がある場合、一時的にメッセージ生成を停止またはレート制限をかける。

6.  **緊急ロールバック/フェイルオーバー**:
    *   最近のデプロイや設定変更が原因である可能性が高い場合、直前の安定バージョンへのロールバックを検討します。
    *   冗長構成が取られている場合、健全なノードへのフェイルオーバーやトラフィックの切り替えを試みます。

## 恒久的な解決策/予防策

将来的に同様の基盤サービス障害を防ぐための、より根本的な解決策とアーキテクチャの改善案を以下に提案します。

1.  **監視とアラートの強化**:
    *   **多角的なメトリクス監視**: データベースの接続数、クエリ実行時間、リソース使用率（CPU, メモリ, I/O）、ディスク使用率、メッセージキューの滞留数、エラーレート、コンシューマの処理速度など、基盤サービスの主要なメトリクスを詳細に監視します。
    *   **閾値ベースのアラート**: 各メトリクスに対して、警告・クリティカルの閾値を設定し、異常を早期に検知できるアラートシステムを構築します。特にディスク容量は、80%で警告、90%でクリティカルなど、段階的なアラートを設定します。
    *   **ログの集約と分析**: 全てのサービスログを中央集約型ログシステム（ELK Stack, Splunk, Datadog Logsなど）に集約し、エラーパターン、異常なアクセス、パフォーマンス低下の兆候を自動で検知・分析できる仕組みを導入します。

2.  **リソースのスケーリングと最適化**:
    *   **データベースの最適化とスケーリング**:
        *   スロークエリの定期的な分析と最適化、インデックスの適切な利用。
        *   リードレプリカの導入による読み込み負荷分散。
        *   データベース接続プールのサイズを適切に設定し、アプリケーションからの接続要求を効率的に処理できるようにする。
        *   必要に応じて、データベースの垂直・水平スケーリングを計画・実行する。
    *   **ストレージの管理と自動化**:
        *   ディスク容量の自動拡張機能（クラウドサービスの場合）の利用。
        *   ログローテーションの自動化と、古いログのアーカイブ・削除ポリシーの徹底。
        *   アプリケーションが生成する一時ファイルのライフサイクル管理とクリーンアップ処理の実装。
        *   ファイルシステムのスナップショットやバックアップの定期的な取得。
    *   **メッセージキューの堅牢化**:
        *   メッセージキューブローカーのクラスタリングや冗長化による高可用性確保。
        *   コンシューマアプリケーションのオートスケーリング設定により、メッセージ量に応じた処理能力を動的に調整。
        *   メッセージのバッチ処理や非同期処理の最適化。

3.  **堅牢なエラーハンドリングとリトライメカニズム**:
    *   **アプリケーションレベルのリトライ**: データベース接続や外部サービス呼び出しにおいて、指数バックオフなどの戦略を用いたリトライロジックを実装し、一時的な障害からの回復力を高めます。
    *   **デッドレターキュー（DLQ）の導入**: メッセージキューにおいて、処理に失敗したメッセージを自動的にDLQに転送する仕組みを導入し、本流の処理をブロックしないようにします。DLQのメッセージは後で手動または自動で再処理・分析できるようにします。
    *   **データバリデーションの強化**: メッセージをキューに送信する前、およびキューから受信した後に、厳格なデータバリデーションを実施し、`malformed JSON`のような不正なデータがシステムに流入・処理されることを防ぎます。

4.  **インフラの冗長化と高可用性設計**:
    *   **単一障害点の排除**: `node-A`のような特定のサーバーに依存するサービスを特定し、冗長化（例: クラスタリング、ロードバランシング、マルチAZデプロイ）を徹底します。
    *   **データベースのクラスタリング/レプリケーション**: データベースの高可用性を確保するため、クラスタリング構成やレプリケーションを導入し、フェイルオーバー機能を強化します。
    *   **ネットワークの冗長化**: ネットワーク機器やパスの冗長化により、ネットワーク障害の影響を最小限に抑えます。

5.  **定期的なメンテナンスとキャパシティプランニング**:
    *   **定期メンテナンス**: データベースのインデックス再構築、統計情報の更新、不要データのパージなどを定期的に実施し、パフォーマンスを維持します。
    *   **キャパシティプランニング**: 定期的にシステムのリソース使用状況をレビューし、将来の成長予測に基づいてリソース増強計画を立て、事前に対応します。
    *   **ログと一時ファイルのクリーンアップ**: 定期的なクリーンアップスクリプトの実行や、ログローテーション設定の見直しにより、ディスク容量の枯渇を予防します。

6.  **開発プロセスとテストの改善**:
    *   **コードレビューの強化**: データベースアクセス層やメッセージング処理部分のコードレビューを徹底し、パフォーマンス問題やエラーハンドリングの不備を早期に発見します。
    *   **負荷テストとカオスエンジニアリング**: 定期的に負荷テストを実施し、システムのボトルネックを特定します。カオスエンジニアリングを導入し、意図的に障害を発生させることで、システムの回復力と監視体制を検証します。
    *   **データ生成と消費の契約**: メッセージキューを介したデータ連携において、データスキーマの厳密な定義（例: JSON Schema）と、それに基づくバリデーションを開発プロセスに組み込みます。